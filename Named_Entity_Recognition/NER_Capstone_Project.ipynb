{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "In this project, the goal is to creat a good machine learning model to classify named entities in the text. This is an information extraction project where we try to extract the following classes: \n",
    "-  geo = Geographical Entity\n",
    "-  org = Organization\n",
    "-  per = Person\n",
    "-  gpe = Geopolitical Entity\n",
    "-  tim = Time Indicator\n",
    "-  art = Artifact\n",
    "-  eve = Event\n",
    "-  nat = Natural Phenomenon\n",
    "\n",
    "The given dataset is an annotated corpus from GMB (Groningen Meaning Bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "\n",
    "The code cell below loads necessary Python libraries and load the annotated corpus data. Note that the last column from this dataset, `'Tag'`, will be our target label (the category to which an individual word belongs to). The first column is an index and can be ignored. Rest of the columns correspond to the `'sentence'`, `'Word'`, and the Part-of-speech `'POS'` notation respectively - These are the features about each individual word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Sentence #           Word  POS Tag\n",
       "0           0         1.0      Thousands  NNS   O\n",
       "1           1         1.0             of   IN   O\n",
       "2           2         1.0  demonstrators  NNS   O\n",
       "3           3         1.0           have  VBP   O\n",
       "4           4         1.0        marched  VBN   O"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import functions as fn\n",
    "\n",
    "from IPython.display import display  # Allows the use of display() for DataFrames\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_table('dataset_05-22-2018.txt')\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "display(data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move onto the models, first step is to look for missing values. The following code cell calculates the number of missing values in each colum. In this dataset, every feature and label are available and so there is no need to worry about missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "Sentence #    0\n",
       "Word          0\n",
       "POS           0\n",
       "Tag           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_count = data.isnull().sum()\n",
    "missing_values_count[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 2999\n",
      "Total number of words: 66161\n"
     ]
    }
   ],
   "source": [
    "n_words = data['Word'].count()\n",
    "n_sentences = data['Sentence #'].nunique()\n",
    "\n",
    "print(\"Total number of sentences: {}\".format(n_sentences))\n",
    "print(\"Total number of words: {}\".format(n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>66161</td>\n",
       "      <td>66161</td>\n",
       "      <td>66161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8766</td>\n",
       "      <td>41</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>the</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3292</td>\n",
       "      <td>9307</td>\n",
       "      <td>56217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word    POS    Tag\n",
       "count   66161  66161  66161\n",
       "unique   8766     41     17\n",
       "top       the     NN      O\n",
       "freq     3292   9307  56217"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 8766 unique words & 9307 nouns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O        0.849700\n",
       "B-geo    0.031287\n",
       "B-org    0.018697\n",
       "I-per    0.018651\n",
       "B-gpe    0.018591\n",
       "B-tim    0.017533\n",
       "B-per    0.016732\n",
       "I-org    0.013996\n",
       "I-geo    0.006257\n",
       "I-tim    0.005048\n",
       "B-art    0.000801\n",
       "B-eve    0.000680\n",
       "I-eve    0.000559\n",
       "I-art    0.000514\n",
       "I-gpe    0.000514\n",
       "B-nat    0.000302\n",
       "I-nat    0.000136\n",
       "Name: Tag, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tag'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "85% of the words does not come under 'Named entities'. This is basically a classification problem and since there is lot of skewness in the data, it is not right to consider \"Prediction Accuracy\" as a validation metric. Simply predicting every word as 'O' would give an accuracy of 85% but it's of no real use. We will look at F1-score as a validation metric (this is the harmonic mean of Precision & Recall).\n",
    "\n",
    "-  Precision answers the question: \"What proportion of positive identifications are actually correct?\" [higher the False-Positives lower the Precision]\n",
    "\n",
    "It is a ratio of true positives(words classified as entities, and which are actually entitites) to all positives(all words classified as entities, irrespective of whether that was the correct classificatio), in other words it is the ratio of\n",
    "\n",
    "`[True Positives/(True Positives + False Positives)]`\n",
    "\n",
    "\n",
    "-  Recall answers the question: \"What proportion of actual positives are identified correctly?\" [higher the False-Negatives, lower the Recall]\n",
    "\n",
    "It is a ratio of true positives(words classified as entities, and which are actually entities) to all the words that were actually entities, in other words it is the ratio of\n",
    "\n",
    "`[True Positives/(True Positives + False Negatives)]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "Before we proceed with feature generation and machine learning algorithms, we need a baseline model. A simplest model would be a 'memorization algorithm'. i.e. just remember the most common named entity for every word and predict that. Incase, we don't know that word, predict 'O'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MemoryTagger(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Expects a list of words as X and a list of tags as y.\n",
    "        '''\n",
    "        voc = {}\n",
    "        self.tags = []\n",
    "        for x, t in zip(X, y):\n",
    "            if t not in self.tags:\n",
    "                self.tags.append(t)\n",
    "            if x in voc:\n",
    "                if t in voc[x]:\n",
    "                    voc[x][t] += 1\n",
    "                else:\n",
    "                    voc[x][t] = 1\n",
    "            else:\n",
    "                voc[x] = {t: 1}\n",
    "        self.memory = {}\n",
    "        for k, d in voc.items():\n",
    "            self.memory[k] = max(d, key=d.get)\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        '''\n",
    "        Predict the the tag from memory. If word is unknown, predict 'O'.\n",
    "        '''\n",
    "        return [self.memory.get(x, 'O') for x in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a multi-class classification problem and hence it is good to look at the precision and recall for each class. This is achieved using `classification_report`. Also, report is printed and the over all F1-score is calculated excluding the non-entity class `O`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      I-art       0.00      0.00      0.00        34\n",
      "      B-gpe       0.86      0.71      0.77      1230\n",
      "      B-art       0.00      0.00      0.00        53\n",
      "      I-per       0.73      0.42      0.53      1234\n",
      "      I-eve       0.42      0.30      0.35        37\n",
      "      B-org       0.59      0.41      0.49      1237\n",
      "      B-eve       0.18      0.09      0.12        45\n",
      "      B-geo       0.69      0.70      0.69      2070\n",
      "      I-org       0.66      0.46      0.54       926\n",
      "      I-geo       0.56      0.46      0.51       414\n",
      "      B-nat       0.44      0.20      0.28        20\n",
      "      I-tim       0.36      0.19      0.25       334\n",
      "      I-gpe       0.50      0.09      0.15        34\n",
      "      B-tim       0.87      0.75      0.81      1160\n",
      "      I-nat       0.80      0.44      0.57         9\n",
      "      B-per       0.75      0.53      0.62      1107\n",
      "\n",
      "avg / total       0.70      0.55      0.61      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "words = data['Word'].values.tolist()\n",
    "tags = data['Tag'].values.tolist()\n",
    "\n",
    "labels = ['I-art', 'B-gpe', 'B-art', 'I-per', 'I-eve', \\\n",
    "          'B-org', 'B-eve', 'B-geo', 'I-org', 'I-geo', \\\n",
    "          'B-nat', 'I-tim', 'I-gpe', 'B-tim', 'I-nat', 'B-per']\n",
    "\n",
    "pred = cross_val_predict(estimator=fn.MemoryTagger(), X=words, y=tags, cv=10)\n",
    "base_model = classification_report(y_pred = pred, y_true = tags, labels=labels)\n",
    "print(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the memorization model, `'Precision'` for named entities is good but `'Recall'` is low implying relatively large amount of 'False Negatives'. This makes sense as our model inherently is designed that way - we assign 'O', if there is no memory of that word which is a 'False Negative'. This acts as our baseline model and now we will perform feature engineering apply machine learning algorithms to get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generation\n",
    "\n",
    "In NLP, some of the important features are:\n",
    "1. Words\n",
    "     -  Current word (essentially like a learned dictionary)\n",
    "     -  previous/previous-previous/next/next-next words (helps in understanding the context)\n",
    "2. Linguistic classification\n",
    "     -  Part-of-speech tags (POS)\n",
    "3. Word substrings (particularly useful with scientific/biological terms)\n",
    "4. Word shapes\n",
    "     -  length, capitalization, titlecase, numerals etc.,\n",
    "5. lemma - root stem, i.e often times the word you end up with, is not something you look up in a dictionary, but you can look up a lemma. Prev/next lemma also gives information about the context\n",
    "\n",
    "Some of the above mentioned features are extracted for the words in the dataset in next few code cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # As default pos in lemmatization is Noun\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "pos = data['POS'].values.tolist()\n",
    "lemma = [lemmatizer.lemmatize(words[i].decode('8859'),get_wordnet_pos(pos[i])) for i in range(len(words))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of Natural Language Processing is to do some form of analysis, or processing, where the machine can understand, at least to some level, what the text means, says, or implies. One of the major forms of pre-processing is filtering out useless data. Often times some words carry more meaning than the others; we can also see that some words are plain useless and we use them as filler words, ex: of, the, a, an etc., In natural language processing, useless words (data), are referred to as stop words. In the nltk.corpus, there are a set of words that are defined as 'stopwords'. All of these words are non-objects and hence I feel this can be a useful feature. Nearly 30% of the total data set are 'stopwords'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_word = [bool(True) if i in stop_words else bool(False) for i in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['stop_word'] = pd.Series(stop_word)\n",
    "data['lemma'] = pd.Series(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>stop_word</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>False</td>\n",
       "      <td>Thousands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>False</td>\n",
       "      <td>demonstrator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "      <td>True</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>False</td>\n",
       "      <td>march</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Sentence #           Word  POS Tag  stop_word         lemma\n",
       "0           0         1.0      Thousands  NNS   O      False     Thousands\n",
       "1           1         1.0             of   IN   O       True            of\n",
       "2           2         1.0  demonstrators  NNS   O      False  demonstrator\n",
       "3           3         1.0           have  VBP   O       True          have\n",
       "4           4         1.0        marched  VBN   O      False         march"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the data is given in terms of annotated words. Using words alone while splitting into test/train sets will result in data bleeding. To avoid this, first we need to convert words into sentences. This is implemented using a class `SentenceGetter` in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, k, l, t) for w, p, k, l, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                                       s[\"POS\"].values.tolist(),\n",
    "                                                                       s[\"lemma\"].values.tolist(),\n",
    "                                                                       s[\"stop_word\"].values.tolist(),\n",
    "                                                                       s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "getter = SentenceGetter(data)\n",
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Thousands', 'NNS', u'Thousands', False, 'O'), ('of', 'IN', u'of', True, 'O'), ('demonstrators', 'NNS', u'demonstrator', False, 'O'), ('have', 'VBP', u'have', True, 'O'), ('marched', 'VBN', u'march', False, 'O'), ('through', 'IN', u'through', True, 'O'), ('London', 'NNP', u'London', False, 'B-geo'), ('to', 'TO', u'to', True, 'O'), ('protest', 'VB', u'protest', False, 'O'), ('the', 'DT', u'the', True, 'O'), ('war', 'NN', u'war', False, 'O'), ('in', 'IN', u'in', True, 'O'), ('Iraq', 'NNP', u'Iraq', False, 'B-geo'), ('and', 'CC', u'and', True, 'O'), ('demand', 'VB', u'demand', False, 'O'), ('the', 'DT', u'the', True, 'O'), ('withdrawal', 'NN', u'withdrawal', False, 'O'), ('of', 'IN', u'of', True, 'O'), ('British', 'JJ', u'British', False, 'B-gpe'), ('troops', 'NNS', u'troop', False, 'O'), ('from', 'IN', u'from', True, 'O'), ('that', 'DT', u'that', True, 'O'), ('country', 'NN', u'country', False, 'O'), ('.', '.', u'.', False, 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different features, as mentioned earlier, corresponding to each word are extracted using the functions in the following code cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2features(data, i):\n",
    "    \n",
    "    ''' Expects a sentence (as printed above) and the index for the word\n",
    "    as input and returns the features corresponding to that word as a dict'''\n",
    "    \n",
    "    Word = data[i][0]\n",
    "    POS = data[i][1]\n",
    "    lemma = data[i][2]\n",
    "    stop_word = data[i][3]\n",
    "    tag = data[i][4]\n",
    "    features = {\n",
    "        'Word': Word,\n",
    "        'POS': POS,\n",
    "        'lemma': lemma,\n",
    "        'stop_word': stop_word,\n",
    "        'tag': tag,\n",
    "        'lowercase': Word.islower(),\n",
    "        'uppercase': Word.isupper(),\n",
    "        'titlecase': Word.istitle(),\n",
    "        'digit': Word.isdigit(),\n",
    "    }\n",
    "    if i == 1:\n",
    "        prev_word = data[i-1][0]\n",
    "        prev_pos = data[i-1][1]\n",
    "        prev_lemma = data[i-1][2]\n",
    "        features.update({\n",
    "            'prev_word': prev_word,\n",
    "            'prev_pos': prev_pos,\n",
    "            'prev_lemma': prev_lemma,\n",
    "            'prev_prev_word': '__Start2__',\n",
    "            'prev_prev_pos': '__Start2__',\n",
    "            'prev_prev_lemma': '__Start2__',\n",
    "            'lowercase_prev': prev_word.islower(),\n",
    "            'titlecase_prev': prev_word.istitle(),\n",
    "            'uppercase_prev': prev_word.isupper(),\n",
    "            'digit_prev': prev_word.isdigit(),\n",
    "        })\n",
    "    elif i > 1:\n",
    "        prev_word = data[i-1][0]\n",
    "        \n",
    "        features.update({\n",
    "            'prev_word': prev_word,\n",
    "            'prev_pos': data[i-1][1],\n",
    "            'prev_lemma': data[i-1][2],\n",
    "            'prev_prev_word': data[i-2][0],\n",
    "            'prev_prev_pos': data[i-2][1],\n",
    "            'prev_prev_lemma':data[i-2][2],\n",
    "            'titlecase_prev': prev_word.istitle(),\n",
    "            'lowercase_prev': prev_word.islower(),\n",
    "            'uppercase_prev': prev_word.isupper(),\n",
    "            'digit_prev': prev_word.isdigit(),\n",
    "        })\n",
    "\n",
    "    elif i == 0:\n",
    "        \n",
    "        features.update({\n",
    "            'prev_word': '__Start1__',\n",
    "            'prev_pos': '__Start1__',\n",
    "            'prev_lemma': '__Start1__',\n",
    "            'prev_prev_word': '__Start1__',\n",
    "            'prev_prev_pos': '__Start1__',\n",
    "            'prev_prev_lemma': '__Start1__',\n",
    "            'lowercase_prev': '__Start1__',\n",
    "            'titlecase_prev': '__Start1__',\n",
    "            'uppercase_prev': '__Start1__',\n",
    "            'digit_prev': '__Start1__',\n",
    "        })\n",
    "\n",
    "\n",
    "    if i < len(data)-2:\n",
    "        next_word = data[i+1][0]\n",
    "        \n",
    "        features.update({\n",
    "            'next_word': next_word,\n",
    "            'next_pos': data[i+1][1],\n",
    "            'next_lemma': data[i+1][2],\n",
    "            'next_next_word': data[i+2][0],\n",
    "            'next_next_lemma': data[i+2][2],\n",
    "            'next_next_pos': data[i+2][1],\n",
    "            'titlecase_next': next_word.istitle(),\n",
    "            'uppercase_next': next_word.isupper(),\n",
    "            'lowercase_next': next_word.islower(),\n",
    "            'digit_next': next_word.isdigit(),\n",
    "        })\n",
    "\n",
    "    elif i == (len(data)-2):\n",
    "        next_word = data[i+1][0]\n",
    "        \n",
    "        features.update({\n",
    "            'next_word': next_word,\n",
    "            'next_pos': data[i+1][1],\n",
    "            'next_lemma': data[i+1][2],\n",
    "            'next_next_word': '__End2__',\n",
    "            'next_next_lemma': '__End2__',\n",
    "            'next_next_pos': '__End2__',\n",
    "            'lowercase_next': next_word.islower(),\n",
    "            'titlecase_next': next_word.istitle(),\n",
    "            'uppercase_next': next_word.isupper(),\n",
    "            'digit_next': next_word.isdigit(),\n",
    "        })\n",
    "\n",
    "    elif i == (len(data)-1):\n",
    "        \n",
    "        features.update({\n",
    "            'next_word': '__End1',\n",
    "            'next_pos': '__End1',\n",
    "            'next_lemma': '__End1',\n",
    "            'next_next_word': '__End1__',\n",
    "            'next_next_lemma': '__End1__',\n",
    "            'next_next_pos': '__End1__',\n",
    "            'lowercase_next': '__End1__',\n",
    "            'titlecase_next': '__End1__',\n",
    "            'uppercase_next': '__End1__',\n",
    "            'digit_next': '__End1__',\n",
    "        })\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 total after feature engineering (including 'Tag' column).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS</th>\n",
       "      <th>Word</th>\n",
       "      <th>digit</th>\n",
       "      <th>digit_next</th>\n",
       "      <th>digit_prev</th>\n",
       "      <th>lemma</th>\n",
       "      <th>lowercase</th>\n",
       "      <th>lowercase_next</th>\n",
       "      <th>lowercase_prev</th>\n",
       "      <th>next_lemma</th>\n",
       "      <th>...</th>\n",
       "      <th>prev_prev_word</th>\n",
       "      <th>prev_word</th>\n",
       "      <th>stop_word</th>\n",
       "      <th>tag</th>\n",
       "      <th>titlecase</th>\n",
       "      <th>titlecase_next</th>\n",
       "      <th>titlecase_prev</th>\n",
       "      <th>uppercase</th>\n",
       "      <th>uppercase_next</th>\n",
       "      <th>uppercase_prev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21522</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>__End1__</td>\n",
       "      <td>False</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>__End1__</td>\n",
       "      <td>False</td>\n",
       "      <td>__End1</td>\n",
       "      <td>...</td>\n",
       "      <td>Red</td>\n",
       "      <td>Sea</td>\n",
       "      <td>False</td>\n",
       "      <td>O</td>\n",
       "      <td>False</td>\n",
       "      <td>__End1__</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>__End1__</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49967</th>\n",
       "      <td>VBN</td>\n",
       "      <td>released</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>release</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>...</td>\n",
       "      <td>a</td>\n",
       "      <td>statement</td>\n",
       "      <td>False</td>\n",
       "      <td>O</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53599</th>\n",
       "      <td>DT</td>\n",
       "      <td>the</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>the</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>virus</td>\n",
       "      <td>...</td>\n",
       "      <td>exposure</td>\n",
       "      <td>to</td>\n",
       "      <td>True</td>\n",
       "      <td>O</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14914</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>,</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>but</td>\n",
       "      <td>...</td>\n",
       "      <td>in</td>\n",
       "      <td>1954</td>\n",
       "      <td>False</td>\n",
       "      <td>O</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43514</th>\n",
       "      <td>DT</td>\n",
       "      <td>the</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>the</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>city</td>\n",
       "      <td>...</td>\n",
       "      <td>part</td>\n",
       "      <td>of</td>\n",
       "      <td>True</td>\n",
       "      <td>O</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       POS      Word  digit digit_next digit_prev    lemma  lowercase  \\\n",
       "21522    .         .  False   __End1__      False        .      False   \n",
       "49967  VBN  released  False      False      False  release       True   \n",
       "53599   DT       the  False      False      False      the       True   \n",
       "14914    ,         ,  False      False       True        ,      False   \n",
       "43514   DT       the  False      False      False      the       True   \n",
       "\n",
       "      lowercase_next lowercase_prev next_lemma      ...       prev_prev_word  \\\n",
       "21522       __End1__          False     __End1      ...                  Red   \n",
       "49967          False           True   Thursday      ...                    a   \n",
       "53599           True           True      virus      ...             exposure   \n",
       "14914           True          False        but      ...                   in   \n",
       "43514           True           True       city      ...                 part   \n",
       "\n",
       "       prev_word stop_word tag titlecase titlecase_next titlecase_prev  \\\n",
       "21522        Sea     False   O     False       __End1__           True   \n",
       "49967  statement     False   O     False           True          False   \n",
       "53599         to      True   O     False          False          False   \n",
       "14914       1954     False   O     False          False          False   \n",
       "43514         of      True   O     False          False          False   \n",
       "\n",
       "      uppercase uppercase_next uppercase_prev  \n",
       "21522     False       __End1__          False  \n",
       "49967     False          False          False  \n",
       "53599     False          False          False  \n",
       "14914     False          False          False  \n",
       "43514     False          False          False  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_final = [sent2features(s) for s in sentences]\n",
    "\n",
    "features_transient = [pd.DataFrame.from_dict(sent) for sent in features_final]\n",
    "features = pd.concat(features_transient, ignore_index = True)\n",
    "print(\"{} total after feature engineering (including 'Tag' column).\".format(len(list(features.columns))))\n",
    "display(features.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_final = features.drop('tag', axis=1)\n",
    "Tags = features['tag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the features generated are either boolean expressions or strings. There is no need to perform normalization or any kind of transformation on this data. However, we should use a label encoder or conver all the string into a number respresentation for the algorithm to understand. Here I used LabelEncoder from the preprocessing toolkit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "for column_name in features_final.columns:\n",
    "    if features_final[column_name].dtype == object:\n",
    "        features_final[column_name] = le.fit_transform(features_final[column_name])\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I tried three supervised learning algorithms to see their relative performance: RandomForest, GradientBoosting and Conditional random field classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      I-art       0.25      0.03      0.05        34\n",
      "      B-gpe       0.69      0.67      0.68      1230\n",
      "      B-art       0.00      0.00      0.00        53\n",
      "      I-per       0.70      0.82      0.76      1234\n",
      "      I-eve       0.17      0.08      0.11        37\n",
      "      B-org       0.54      0.47      0.50      1237\n",
      "      B-eve       0.35      0.13      0.19        45\n",
      "      B-geo       0.60      0.75      0.66      2070\n",
      "      I-org       0.56      0.50      0.53       926\n",
      "      I-geo       0.52      0.46      0.49       414\n",
      "      B-nat       0.00      0.00      0.00        20\n",
      "      I-tim       0.74      0.41      0.52       334\n",
      "      I-gpe       0.23      0.09      0.13        34\n",
      "      B-tim       0.74      0.59      0.66      1160\n",
      "      I-nat       0.00      0.00      0.00         9\n",
      "      B-per       0.69      0.67      0.68      1107\n",
      "\n",
      "avg / total       0.63      0.62      0.62      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_B = RandomForestClassifier(random_state = 10)\n",
    "y_pred_RF = cross_val_predict(estimator=clf_B, X=features_final, y=Tags, cv=5)\n",
    "report_RF = classification_report(Tags, y_pred_RF, labels=labels)\n",
    "print(report_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      I-art       0.00      0.00      0.00        34\n",
      "      B-gpe       0.76      0.70      0.73      1230\n",
      "      B-art       0.09      0.04      0.05        53\n",
      "      I-per       0.72      0.87      0.79      1234\n",
      "      I-eve       0.28      0.14      0.18        37\n",
      "      B-org       0.63      0.49      0.55      1237\n",
      "      B-eve       0.46      0.13      0.21        45\n",
      "      B-geo       0.62      0.81      0.70      2070\n",
      "      I-org       0.64      0.53      0.58       926\n",
      "      I-geo       0.62      0.49      0.54       414\n",
      "      B-nat       0.25      0.20      0.22        20\n",
      "      I-tim       0.74      0.38      0.50       334\n",
      "      I-gpe       0.16      0.09      0.11        34\n",
      "      B-tim       0.81      0.63      0.71      1160\n",
      "      I-nat       0.40      0.22      0.29         9\n",
      "      B-per       0.71      0.73      0.72      1107\n",
      "\n",
      "avg / total       0.68      0.66      0.66      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf_A = GradientBoostingClassifier(random_state = 10)\n",
    "\n",
    "y_pred_GB = cross_val_predict(estimator=clf_A, X=features_final, y=Tags, cv=5)\n",
    "report_GB = classification_report(Tags, y_pred_GB, labels=labels)\n",
    "print(report_GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Random Field Algorithm\n",
    "\n",
    "To see the performance, a simple CRF algorithm is tried here using the sklearn_crfsuite toolkit. This considers the conditional probabilities in predictions. It is successfully applied in many NLP applications. \n",
    "\n",
    "sklearn_crfsuite requires the inputs to be in the form of dict. Hence slight modification is done for the inputs and is implemented in the follwoing code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2features1(data, i):\n",
    "    \n",
    "    ''' Expects a sentence (as printed above) and the index for the word\n",
    "    as input and returns the features corresponding to that word as a dict'''\n",
    "    \n",
    "    Word = data[i][0]\n",
    "    POS = data[i][1]\n",
    "    lemma = data[i][2]\n",
    "    stop_word = data[i][3]\n",
    "    tag = data[i][4]\n",
    "    features = {\n",
    "        'Word': Word,\n",
    "        'POS': POS,\n",
    "        'lemma': lemma,\n",
    "        'stop_word': stop_word,\n",
    "        'lowercase': Word.islower(),\n",
    "        'uppercase': Word.isupper(),\n",
    "        'titlecase': Word.istitle(),\n",
    "        'digit': Word.isdigit(),\n",
    "    }\n",
    "    if i == 1:\n",
    "        prev_word = data[i-1][0]\n",
    "        prev_pos = data[i-1][1]\n",
    "        prev_lemma = data[i-1][2]\n",
    "        features.update({\n",
    "            'prev_word': prev_word,\n",
    "            'prev_pos': prev_pos,\n",
    "            'prev_lemma': prev_lemma,\n",
    "            'prev_prev_word': '__Start2__',\n",
    "            'prev_prev_pos': '__Start2__',\n",
    "            'prev_prev_lemma': '__Start2__',\n",
    "            'lowercase_prev': prev_word.islower(),\n",
    "            'titlecase_prev': prev_word.istitle(),\n",
    "            'uppercase_prev': prev_word.isupper(),\n",
    "            'digit_prev': prev_word.isdigit(),\n",
    "        })\n",
    "    elif i > 1:\n",
    "        prev_word = data[i-1][0]\n",
    "        \n",
    "        features.update({\n",
    "            'prev_word': prev_word,\n",
    "            'prev_pos': data[i-1][1],\n",
    "            'prev_lemma': data[i-1][2],\n",
    "            'prev_prev_word': data[i-2][0],\n",
    "            'prev_prev_pos': data[i-2][1],\n",
    "            'prev_prev_lemma':data[i-2][2],\n",
    "            'titlecase_prev': prev_word.istitle(),\n",
    "            'lowercase_prev': prev_word.islower(),\n",
    "            'uppercase_prev': prev_word.isupper(),\n",
    "            'digit_prev': prev_word.isdigit(),\n",
    "        })\n",
    "\n",
    "    elif i == 0:\n",
    "        \n",
    "        features.update({\n",
    "            'prev_word': '__Start1__',\n",
    "            'prev_pos': '__Start1__',\n",
    "            'prev_lemma': '__Start1__',\n",
    "            'prev_prev_word': '__Start1__',\n",
    "            'prev_prev_pos': '__Start1__',\n",
    "            'prev_prev_lemma': '__Start1__',\n",
    "            'lowercase_prev': '__Start1__',\n",
    "            'titlecase_prev': '__Start1__',\n",
    "            'uppercase_prev': '__Start1__',\n",
    "            'digit_prev': '__Start1__',\n",
    "        })\n",
    "\n",
    "\n",
    "    if i < len(data)-2:\n",
    "        next_word = data[i+1][0]\n",
    "        \n",
    "        features.update({\n",
    "            'next_word': next_word,\n",
    "            'next_pos': data[i+1][1],\n",
    "            'next_lemma': data[i+1][2],\n",
    "            'next_next_word': data[i+2][0],\n",
    "            'next_next_lemma': data[i+2][2],\n",
    "            'next_next_pos': data[i+2][1],\n",
    "            'titlecase_next': next_word.istitle(),\n",
    "            'uppercase_next': next_word.isupper(),\n",
    "            'lowercase_next': next_word.islower(),\n",
    "            'digit_next': next_word.isdigit(),\n",
    "        })\n",
    "\n",
    "    elif i == (len(data)-2):\n",
    "        next_word = data[i+1][0]\n",
    "        \n",
    "        features.update({\n",
    "            'next_word': next_word,\n",
    "            'next_pos': data[i+1][1],\n",
    "            'next_lemma': data[i+1][2],\n",
    "            'next_next_word': '__End2__',\n",
    "            'next_next_lemma': '__End2__',\n",
    "            'next_next_pos': '__End2__',\n",
    "            'lowercase_next': next_word.islower(),\n",
    "            'titlecase_next': next_word.istitle(),\n",
    "            'uppercase_next': next_word.isupper(),\n",
    "            'digit_next': next_word.isdigit(),\n",
    "        })\n",
    "\n",
    "    elif i == (len(data)-1):\n",
    "        \n",
    "        features.update({\n",
    "            'next_word': '__End1',\n",
    "            'next_pos': '__End1',\n",
    "            'next_lemma': '__End1',\n",
    "            'next_next_word': '__End1__',\n",
    "            'next_next_lemma': '__End1__',\n",
    "            'next_next_pos': '__End1__',\n",
    "            'lowercase_next': '__End1__',\n",
    "            'titlecase_next': '__End1__',\n",
    "            'uppercase_next': '__End1__',\n",
    "            'digit_next': '__End1__',\n",
    "        })\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features1(sent):\n",
    "    return [word2features1(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels1(sent):\n",
    "    return [label for word, pos, lemma, stopword, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YathishPC\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      I-art       0.00      0.00      0.00        34\n",
      "      B-gpe       0.81      0.72      0.76      1230\n",
      "      B-art       0.00      0.00      0.00        53\n",
      "      I-per       0.82      0.90      0.86      1234\n",
      "      I-eve       0.44      0.19      0.26        37\n",
      "      B-org       0.65      0.60      0.63      1237\n",
      "      B-eve       0.56      0.22      0.32        45\n",
      "      B-geo       0.71      0.82      0.76      2070\n",
      "      I-org       0.70      0.73      0.72       926\n",
      "      I-geo       0.68      0.63      0.65       414\n",
      "      B-nat       0.22      0.10      0.14        20\n",
      "      I-tim       0.72      0.50      0.59       334\n",
      "      I-gpe       0.14      0.06      0.08        34\n",
      "      B-tim       0.90      0.80      0.85      1160\n",
      "      I-nat       0.67      0.22      0.33         9\n",
      "      B-per       0.80      0.77      0.78      1107\n",
      "\n",
      "avg / total       0.75      0.74      0.74      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "\n",
    "crf = CRF(algorithm='lbfgs',\n",
    "          c1=0.1,\n",
    "          c2=0.1,\n",
    "          max_iterations=100,\n",
    "          all_possible_transitions=False)\n",
    "\n",
    "X = [sent2features1(s) for s in sentences]\n",
    "y = [sent2labels1(s) for s in sentences]\n",
    "\n",
    "y_pred_crf = cross_val_predict(estimator=crf, X=X, y=y, cv=10)\n",
    "report_crf = flat_classification_report(y_pred = y_pred_crf, y_true=y, labels=labels)\n",
    "print(report_crf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classificationReport2dict(report):\n",
    "    lines = report.split('\\n')\n",
    "    classes = {}\n",
    "    \n",
    "    classes.update({'Precision': lines[19].split()[3],\n",
    "                   'Recall': lines[19].split()[4],\n",
    "                   'F1-Score': lines[19].split()[5],})\n",
    "    \n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Memory Tagger</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conditional Random Field</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Algorithm F1-Score Precision Recall\n",
       "0             Memory Tagger     0.61      0.70   0.55\n",
       "1             Random Forest     0.62      0.63   0.62\n",
       "2         Gradient Boosting     0.66      0.68   0.66\n",
       "3  Conditional Random Field     0.74      0.75   0.74"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = classificationReport2dict(base_model)\n",
    "A.update({'Algorithm':'Memory Tagger',})\n",
    "B = classificationReport2dict(report_RF)\n",
    "B.update({'Algorithm':'Random Forest',})\n",
    "C = classificationReport2dict(report_GB)\n",
    "C.update({'Algorithm':'Gradient Boosting',})\n",
    "D = classificationReport2dict(report_crf)\n",
    "D.update({'Algorithm':'Conditional Random Field',})\n",
    "results = [A,B,C,D]\n",
    "metrics = pd.DataFrame.from_dict(results)\n",
    "display(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall compare RandomForest & GradientBoosting algorithms. MemoryTagger is not a good idea as it just remembers the data. From the cross_validation reports, it is evident (F1-scores) that GradientBoosting algorithm does better than RandomForest. Also the overall average F1-score is 0.66 for GradientBoosting algorithm & 0.63 for RandomForest. Note that class 'O' is excluded in calculating the overall F1-score. Also, one thing to note is that `'Recall'` has improved significantly when ML models are used.\n",
    "CRF does better than RF and GB, but it is not optimized here as it was used in Stanford CoreNLP.\n",
    "\n",
    "Conditional random field algorithm performs much better on the dataset. The precision, recall and over all F1-score imporved to 0.74. Tuning the regularization parameters might yield better results. GridSearch and complexity curves would reveal more information and help in tuning the model for optimum performance. \n",
    "\n",
    "Importance of context in NER, skewness in the data, large volume of data and large number of features involved makes CRF a good algorithm for this particular data set. Also, F1-score is highest for CRF algorith. I will go ahead with CRF and perform a grid search optimization over the entire training set by tuning at least one parameter to improve upon the untuned model's F1-score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NER problems, we shall be careful while doing a test train split. We have to do the split on the sentences so that we don't loose the context and avoid data bleeding. This is implemented in the following cell. The sentences are first split and then every word in those train sets is used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 2399 sentences.\n",
      "Testing set has 600 sentences.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the 'features' and 'income' data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 0)\n",
    "\n",
    "# Show the results of the split\n",
    "print(\"Training set has {} sentences.\".format(len(X_train)))\n",
    "print(\"Testing set has {} sentences.\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning (Grid Search)\n",
    "Inorder to tune the model to get optimal parameters, we can employ techniques like GridSearchCV and RandomizedSearchCV.\n",
    "-  These are techniques that are used to find the optimal hyper parameters for the model\n",
    "-  Select the hyperparameters that might improve the model (based on intuition) and then fit the model with all combinations of parameters. Calculate the evaluation metric F1-score and find the optimal combination. Use that to finally make the model\n",
    "-  In gridsearch, model is validated using k-fold cross-validation: divide the data into k bins and use k-1 bins for training and the remaining bin for testing. We repeat this process k times and finally calculate the average error which will be the generalized error. This will remove/reduce the bias in the data if any. ShuffleSplit is another cross-validation technique which is implemented in this. In the following implementation, it creates 5 shuffled sets, and for each shuffle, 20%('test_size') of data will be used as validation set. It helps to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c2': 0.018117123574024716, 'c1': 0.31760548772707742}\n",
      "Unoptimized model\n",
      "----------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      I-art       0.00      0.00      0.00         7\n",
      "      B-gpe       0.84      0.82      0.83       244\n",
      "      B-art       0.00      0.00      0.00        16\n",
      "      I-per       0.83      0.95      0.89       276\n",
      "      I-eve       1.00      0.33      0.50         6\n",
      "      B-org       0.68      0.62      0.65       256\n",
      "      B-eve       1.00      0.29      0.44         7\n",
      "      B-geo       0.75      0.86      0.80       392\n",
      "      I-org       0.71      0.75      0.73       193\n",
      "      I-geo       0.70      0.63      0.67        68\n",
      "      B-nat       0.00      0.00      0.00         5\n",
      "      I-tim       0.78      0.45      0.57        86\n",
      "      I-gpe       1.00      0.11      0.20         9\n",
      "      B-tim       0.94      0.75      0.84       258\n",
      "      I-nat       0.00      0.00      0.00         5\n",
      "      B-per       0.87      0.85      0.86       255\n",
      "\n",
      "avg / total       0.79      0.77      0.77      2083\n",
      "\n",
      "Optimized model\n",
      "----------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      I-art       0.00      0.00      0.00         7\n",
      "      B-gpe       0.86      0.82      0.84       244\n",
      "      B-art       1.00      0.06      0.12        16\n",
      "      I-per       0.84      0.94      0.89       276\n",
      "      I-eve       0.60      0.50      0.55         6\n",
      "      B-org       0.67      0.62      0.64       256\n",
      "      B-eve       0.71      0.71      0.71         7\n",
      "      B-geo       0.77      0.84      0.80       392\n",
      "      I-org       0.72      0.76      0.74       193\n",
      "      I-geo       0.70      0.62      0.66        68\n",
      "      B-nat       1.00      0.20      0.33         5\n",
      "      I-tim       0.83      0.50      0.62        86\n",
      "      I-gpe       1.00      0.11      0.20         9\n",
      "      B-tim       0.93      0.83      0.88       258\n",
      "      I-nat       1.00      0.20      0.33         5\n",
      "      B-per       0.87      0.85      0.86       255\n",
      "\n",
      "avg / total       0.81      0.78      0.78      2083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn_crfsuite.metrics import flat_f1_score\n",
    "from scipy.stats import expon\n",
    "\n",
    "clf = CRF(algorithm='lbfgs',\n",
    "          max_iterations=100,\n",
    "          all_possible_transitions=True)\n",
    "\n",
    "param_dist = {\"c1\": expon(scale = 0.5),\n",
    "              \"c2\": expon(scale = 0.05)}\n",
    "\n",
    "scorer = make_scorer(flat_f1_score, labels= labels, average = 'weighted')\n",
    "\n",
    "cv_sets = ShuffleSplit(n_splits = 5, test_size = 0.20, random_state = 0)\n",
    "\n",
    "grid_obj = RandomizedSearchCV(clf, param_dist, scoring=scorer, cv=cv_sets, verbose = 0, n_iter = 25)\n",
    "\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "best_parameters = grid_fit.best_params_\n",
    "print(best_parameters)\n",
    "\n",
    "# Make predictions using the unoptimized and optimized model\n",
    "predictions = (clf.fit(X_train, y_train)).predict(X_test)\n",
    "best_predictions = best_clf.predict(X_test)\n",
    "print(\"Unoptimized model\\n----------------------\")\n",
    "print(flat_classification_report(y_test, predictions, labels = labels))\n",
    "print(\"Optimized model\\n----------------------\")\n",
    "print(flat_classification_report(y_test, best_predictions, labels = labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in the calssification report, optimized model has better F1-score and other evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Model Performance\n",
    "\n",
    "### Learning Curves\n",
    "Here, we'll take a look at the models' learning and testing performances on various subsets of training data. Graphing the model's performance based on varying criteria can be beneficial in the analysis process, such as visualizing behavior that may not have been apparent from the results alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def ModelLearning(X, y, regressor):\n",
    "    \"\"\" Calculates the performance of several models with varying sizes of training data.\n",
    "        The learning and testing scores for each model are then plotted. \"\"\"\n",
    "    \n",
    "    \n",
    "    cv = ShuffleSplit(n_splits=5, test_size = 0.2, random_state = 0)\n",
    "    \n",
    "    scorer = make_scorer(flat_f1_score,labels = labels,average = 'weighted')\n",
    "    \n",
    "    train_sizes = np.rint(np.linspace(1, len(X)*0.8 - 1, 9)).astype(int)\n",
    "\n",
    "    sizes, train_scores, test_scores = learning_curve(regressor, X, y, \\\n",
    "            cv = cv, train_sizes = train_sizes, scoring = scorer, verbose=0)\n",
    "        \n",
    "        # Find the mean and standard deviation for smoothing\n",
    "    train_std = np.std(train_scores, axis = 1)\n",
    "    train_mean = np.mean(train_scores, axis = 1)\n",
    "    test_std = np.std(test_scores, axis = 1)\n",
    "    test_mean = np.mean(test_scores, axis = 1)\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(sizes, train_mean, 'o-', color = 'r', label = 'Training Score')\n",
    "    plt.plot(sizes, test_mean, 'o-', color = 'g', label = 'Testing Score')\n",
    "    plt.fill_between(sizes, train_mean - train_std, \\\n",
    "        train_mean + train_std, alpha = 0.15, color = 'r')\n",
    "    plt.fill_between(sizes, test_mean - test_std, \\\n",
    "        test_mean + test_std, alpha = 0.15, color = 'g')\n",
    "        \n",
    "        # Labels\n",
    "    plt.title('Learning Curve')\n",
    "    \n",
    "    # Visual aesthetics\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.xlabel('Number of sentences in Training set')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.ylim([-0.05,1.05])\n",
    "    plt.xlim([0, len(X)*0.8])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb0AAAFNCAYAAACUvLFdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ffa1dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ModelLearning(X,y,best_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above learning curves are generated using weighted average f1-score for all named-entities ('O' is excluded from the f1-score calculation). If 'O' is also included then test f1-score will also be close to train f1-score. These curves are useful in finding out if the model is limited by data. validation score is increasing marginally with increase in data points, but it is not very significant after 2000 sentences. It looks like the algorithm is not limited by data. But we need to be careful about multiple classes that are present. It is possible to look at scores of each class seperately and see how each class is being affected by addition of more data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity Curves\n",
    "The following two code cells produces a graph for a conditional random field model that has been trained and validated on the dataset using different values for the first order regularization parameter 'C1'. The graph produces two complexity curves â€” one for training and one for validation. Similar to the **learning curves**, the shaded regions of both the complexity curves denote the uncertainty in those curves, and the model is scored on both the training and validation sets using the `scorer` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "def ModelComplexity(X, y, best_clf):\n",
    "    \"\"\" Calculates the performance of the model as model complexity increases.\n",
    "        The learning and testing errors rates are then plotted. \"\"\"\n",
    "    \n",
    "    cv = ShuffleSplit(n_splits = 5, test_size = 0.2, random_state = 0)\n",
    "\n",
    "    c1 = np.arange(0.0, 1, 0.05)\n",
    "    \n",
    "    scorer = make_scorer(flat_f1_score, labels = labels, average = 'weighted')    \n",
    "\n",
    "    # Calculate the training and testing scores\n",
    "    train_scores, test_scores = validation_curve(best_clf, X, y, \\\n",
    "        param_name = \"c1\", param_range = c1, cv = cv, scoring = scorer)\n",
    "\n",
    "    # Find the mean and standard deviation for smoothing\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot the validation curve\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.title('CRF Complexity Performance')\n",
    "    plt.plot(c1, train_mean, 'o-', color = 'r', label = 'Training Score')\n",
    "    plt.plot(c1, test_mean, 'o-', color = 'g', label = 'Validation Score')\n",
    "    plt.fill_between(c1, train_mean - train_std, \\\n",
    "        train_mean + train_std, alpha = 0.15, color = 'r')\n",
    "    plt.fill_between(c1, test_mean - test_std, \\\n",
    "        test_mean + test_std, alpha = 0.15, color = 'g')\n",
    "    \n",
    "    # Visual aesthetics\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.xlabel('Regularization parameter, C1')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.ylim([-0.05,1.05])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb0AAAFNCAYAAACUvLFdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niYhIzlDQExGRnKGgJyIiOeP/AzD+5E27y3ApAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x316f4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ModelComplexity(X,y,best_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  The above graph shows the performance of algorithm as we vary the first order regularization parameter, `C1`. Validation score is almost indifferent to the changes in C1. Training score increases as we decrease the C1 and hence the gap between test & train score is increasing. Looking at the training score of close to 1, it looks like the model is getting over fitted. However, we need further exploration to decide on over fitting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows the most likely and most unlikely transitions from one tag to another as per the optimized algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "B-art  -> I-art   7.514900\n",
      "B-eve  -> I-eve   7.158015\n",
      "I-art  -> I-art   6.668138\n",
      "I-eve  -> I-eve   6.067339\n",
      "I-gpe  -> I-gpe   5.507066\n",
      "B-nat  -> I-nat   5.461355\n",
      "B-gpe  -> I-gpe   5.460731\n",
      "B-org  -> I-org   5.260555\n",
      "I-org  -> I-org   4.826151\n",
      "B-geo  -> I-geo   4.743480\n",
      "I-geo  -> I-geo   4.525396\n",
      "O      -> O       4.497626\n",
      "B-per  -> I-per   4.346820\n",
      "I-tim  -> I-tim   3.940976\n",
      "B-tim  -> I-tim   3.706914\n",
      "I-per  -> I-per   3.270650\n",
      "O      -> B-per   2.308072\n",
      "B-gpe  -> B-org   2.117786\n",
      "O      -> B-org   1.752272\n",
      "O      -> B-tim   1.736650\n",
      "\n",
      "Top unlikely transitions:\n",
      "B-geo  -> B-geo   -1.565001\n",
      "B-gpe  -> I-per   -1.582340\n",
      "B-org  -> I-per   -1.867718\n",
      "B-per  -> I-org   -1.873048\n",
      "B-gpe  -> I-geo   -2.037696\n",
      "I-tim  -> B-tim   -2.264695\n",
      "B-geo  -> I-tim   -2.281119\n",
      "B-geo  -> B-per   -2.281602\n",
      "B-geo  -> I-per   -2.408835\n",
      "B-gpe  -> I-org   -2.565429\n",
      "B-geo  -> I-org   -2.637556\n",
      "I-org  -> I-per   -2.866836\n",
      "B-tim  -> B-tim   -3.131238\n",
      "O      -> I-per   -3.214667\n",
      "I-per  -> B-per   -3.430358\n",
      "B-per  -> B-per   -3.735447\n",
      "O      -> I-tim   -4.693059\n",
      "B-gpe  -> B-gpe   -4.719680\n",
      "O      -> I-geo   -5.260520\n",
      "O      -> I-org   -5.304549\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(best_clf.transition_features_).most_common(20))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(best_clf.transition_features_).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results show that algorithm thinks it is highly likely that beginning of an event (B-eve) is followed by a tag inside of an event (I-eve) and such similar things. Also the unlikely transitions are penalized with a negative coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "4.952642 B-tim    Word:Wednesday\n",
      "4.952642 B-tim    lemma:Wednesday\n",
      "4.879709 B-tim    Word:Monday\n",
      "4.879709 B-tim    lemma:Monday\n",
      "4.866381 B-tim    lemma:Thursday\n",
      "4.866381 B-tim    Word:Thursday\n",
      "4.818657 B-tim    lemma:Tuesday\n",
      "4.818657 B-tim    Word:Tuesday\n",
      "4.639730 B-tim    lemma:Sunday\n",
      "4.639730 B-tim    Word:Sunday\n",
      "\n",
      "Top negative:\n",
      "-2.037779 O        prev_word:Rio\n",
      "-2.037779 O        prev_lemma:Rio\n",
      "-2.085354 O        Word:end\n",
      "-2.161337 I-per    prev_prev_pos:DT\n",
      "-2.219919 O        lemma:decade\n",
      "-2.285546 B-geo    prev_lemma:recognize\n",
      "-2.769281 I-gpe    prev_pos:JJ\n",
      "-2.821084 B-gpe    lowercase\n",
      "-3.048897 O        Word:day\n",
      "-4.950461 O        POS:NNP\n"
     ]
    }
   ],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(best_clf.state_features_).most_common(10))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(best_clf.state_features_).most_common()[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look at the top positive state features reveal that the algorithm remembers some of the words or lemmas and try to predict them. The algorithm seems to overfit and probably we need to use better features. One solution to this problem is to carry out principal component analysis on the features and then apply CRF on the most important newly generated compound features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions:\n",
    "\n",
    "-  Over all performance of the model from `learning curves` shows that we are not data-limited. But, it is seen that 85% of the data are non-entities and hence create noise in the model. I think, an alternative way is to remove all the words that are classified as `stop_words` (~31%) and then train the algorithm on remaining data - this would reduce the noise and also over-all F1-score would be more meaningful. \n",
    "-  As we observed, the CRF algorithm with the generated features seems to overfit the data - can overcome this by doing PCA on the feature set before applying CRF. Significant improvement form the memory_tagger is seen particularly in Recall. \n",
    "-  For better performance, we can go for other perceptron and deep neural networks - publicly available libraries for NER like Spacy and Stanford CoreNLP will also improve the algorithm performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
